# -*- coding: utf-8 -*-
"""Employee Salary Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzgsSZcRmibbTiMM1PKrxkWzb_MeE_tE
"""

# Data Handling and Visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Machine Learning Imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""DATA LOADING

"""

# Load the data
data = pd.read_csv('adult.csv')
data.head(15)

"""INITIAL INSPECTION"""

# Check shape and first look
print(data.shape)
print(data.columns)
data.head()

print(data.columns)

"""NULL AND MISSING VALUE INSPECTION"""

# Check for nulls in each column
data.isna().sum()

# Inspect value counts for crucial categorical features
print(data.workclass.value_counts())
print(data.education.value_counts())
print(data['marital-status'].value_counts())
print(data.occupation.value_counts())

"""DATA CLEANING"""

# Replace missing/unknown values
data.workclass.replace({'?': 'Others'}, inplace=True)
data.occupation.replace({'?': 'Others'}, inplace=True)
print("Workclass:\n", data['workclass'].value_counts())
print("Occupation:\n", data['occupation'].value_counts())

"""OUTLIER HANDLING"""

# Boxplot for age
plt.boxplot(data['age'])
plt.title('Age Outlier Detection')
plt.show()

data = data[(data['age'] <= 75) & (data['age'] >= 17)]

""" REMOVAL OF SPARSE/IRRELEVANT CATEGORIES"""

# Remove rare job classes
data = data[(data['workclass'] != 'Without-pay') & (data['workclass'] != 'Never-worked')]
print(data.workclass.value_counts())

data = data[data['education'] != '1st-4th']
data = data[data['education'] != '5th-6th']
data = data[data['education'] != 'Preschool']
print(data.education.value_counts())

"""ENCODING CATEGORICAL FEATURES"""

# Encode categorical columns numerically
encoder = LabelEncoder()
categorical_cols = [
    'workclass', 'marital-status', 'occupation',
    'relationship', 'race', 'gender', 'native-country'
]
for col in categorical_cols:
    data[col] = encoder.fit_transform(data[col])
data.head()

"""EXPLORING DATA ANALYSIS AND VISUALIZATION"""

# Correlation Heatmap (only numeric columns)
plt.figure(figsize=(12, 8))
numeric_data = data.select_dtypes(include=['int64', 'float64'])  # Fix: select only numeric columns
sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# Income Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='income', data=data)
plt.title("Income Class Distribution")
plt.xlabel("Income Category")
plt.ylabel("Count")
plt.show()

# Age Distribution by Income
plt.figure(figsize=(8, 5))
sns.boxplot(x='income', y='age', data=data)
plt.title("Age Distribution by Income Group")
plt.xlabel("Income Category")
plt.ylabel("Age")
plt.show()

# Educational-num vs. Age, separated by Income
plt.figure(figsize=(10, 5))
sns.barplot(x='educational-num', y='age', hue='income', data=data, ci=None)
plt.title("Educational Level vs. Age by Income")
plt.xlabel("Education Number")
plt.ylabel("Average Age")
plt.legend(title='Income')
plt.show()

# Workclass Counts
plt.figure(figsize=(10, 5))
sns.countplot(y='workclass', data=data, order=data['workclass'].value_counts().index)
plt.title("Workclass Distribution")
plt.xlabel("Count")
plt.ylabel("Workclass")
plt.show()

# Hours Worked per Week by Income
plt.figure(figsize=(8, 5))
sns.boxplot(x='income', y='hours-per-week', data=data)
plt.title("Hours per Week by Income Group")
plt.xlabel("Income Category")
plt.ylabel("Hours per Week")
plt.show()

"""FEATURE SCALING"""

# Split into X, Y (features, target)
X = data.drop(columns=['income'])
Y = data['income']

X_encoded = pd.get_dummies(X)

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_encoded)

"""TT SPLIT"""

# Drop rows with missing values before scaling
X_encoded = X_encoded.dropna()
Y = Y[X_encoded.index]  # Align target variable

# Apply scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Now safely split
xtrain, xtest, ytrain, ytest = train_test_split(
    X_scaled, Y, test_size=0.2, random_state=23, stratify=Y
)

"""KNN MODEL TRAINING"""

# Train KNN classifier
knn = KNeighborsClassifier()
knn.fit(xtrain, ytrain)
predict = knn.predict(xtest)

"""IMPORTING VARIOUS MODEL LIBRARIES"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

"""Multiple Models Review"""

models = {
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC()
}

print("üìä Accuracy of Different Models:\n")

for name, model in models.items():
    model.fit(xtrain, ytrain)
    preds = model.predict(xtest)
    acc = accuracy_score(ytest, preds)
    print(f"{name}: {acc:.4f}")

best_model = None
best_score = 0

for name, model in models.items():
    model.fit(xtrain, ytrain)
    preds = model.predict(xtest)
    acc = accuracy_score(ytest, preds)
    if acc > best_score:
        best_score = acc
        best_model = model
    print(f"{name}: {acc:.4f}")

"""MODEL SAVING"""

import pickle
with open("best_model.pkl", "wb") as f:
    pickle.dump(RandomForestClassifier().fit(xtrain, ytrain), f)

"""PREDICTION FORMAT CHECK

"""

sample_input = [xtrain[0]]
print(best_model.predict(sample_input))

"""BEST MODEL SELECTION"""

# Save the Best Model
import pickle

best_model = RandomForestClassifier()
best_model.fit(xtrain, ytrain)

with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print("‚úÖ Model saved as 'best_model.pkl'")

import joblib

# Assume your trained model is called `best_model`
joblib.dump(best_model, 'best_model.pkl')

import joblib

# Compress level 3 is usually a good trade-off
joblib.dump(best_model, 'best_model.pkl', compress=3)

import pickle

# After training best_model
with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

from sklearn.ensemble import RandomForestClassifier
import pickle

# Train smaller Random Forest model (fewer trees + lower depth)
best_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
best_model.fit(xtrain, ytrain)

# Save it as a new pickle file
with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print("‚úÖ New smaller model saved.")

from google.colab import files
files.download("best_model.pkl")

import pickle

# Save feature names after one-hot encoding
with open("columns.pkl", "wb") as f:
    pickle.dump(X_encoded.columns, f)

"""MODEL EVALUATION"""

# Accuracy
print("Accuracy:", accuracy_score(ytest, predict))

print(classification_report(ytest, predict))

cm = confusion_matrix(ytest, predict)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""HYPERPARAMETED TUNING

"""

# from sklearn.model_selection import GridSearchCV
# param_grid = {
#     'n_neighbors': [3, 5, 7],
#     'weights': ['uniform', 'distance'],
#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
# }
# gsv = GridSearchCV(KNeighborsClassifier(), param_grid, n_jobs=-1, refit=True)
# gsv.fit(xtrain, ytrain)
# best_pred = gsv.predict(xtest)
# print("Best Params:", gsv.best_params_)
# print("Best CV Score:", gsv.best_score_)
# print("Accuracy (Tuned):", accuracy_score(ytest, best_pred))

"""# üíº Project: Employee Salary Prediction

## üìå Objective:
To build a machine learning model that predicts whether an employee earns more than $50K/year based on personal and work-related attributes using the UCI Adult Census dataset.

## üîç Steps Performed:

1. **Data Loading**: Imported the dataset using pandas.
2. **Initial Exploration**: Checked shape, columns, and types of data.
3. **Missing Value Handling**: Replaced '?' with 'Others', cleaned invalid values.
4. **Outlier Detection**: Boxplot for age; filtered unreasonable values.
5. **Category Filtering**: Removed non-informative categories (e.g. Preschool).
6. **Encoding**: Converted categorical fields using LabelEncoder.
7. **Data Visualization (EDA)**:
   - Correlation heatmap
   - Income distribution
   - Workclass & education analysis
8. **Feature Scaling**: Normalized all features using MinMaxScaler.
9. **Train-Test Split**: Data divided for training and evaluation (80/20).
10. **Model Training**:
    - Implemented 5 models: KNN, Random Forest, SVM, Logistic Regression, Decision Tree.
    - Evaluated with accuracy and classification report.
11. **Best Model Saved**: Trained Random Forest and saved it using `pickle`.
12. **Deployment Ready**: Output format verified for Streamlit app.

## üéØ Outcome:
Achieved high accuracy with Random Forest. Project ready for web deployment using Streamlit.

---

‚úÖ Prepared for internship submission, GitHub showcase, and real-world deployment.

# ‚úÖ Conclusion & Ready for Deployment

This project walks through the complete machine learning pipeline to predict whether an employee earns more than $50K annually based on demographic and work-related features using the UCI Adult dataset.

## üîç What We Did:
- Cleaned the dataset and handled missing values like '?'
- Removed outliers and rare/irrelevant entries
- Encoded categorical features numerically
- Visualized relationships between features and income
- Trained and compared 5 ML models: KNN, Random Forest, Logistic Regression, SVM, Decision Tree
- Chose the best model (Random Forest) and saved it using `pickle`
- Confirmed input format readiness for Streamlit web app

## üöÄ Ready for Deployment:
- Model saved as `best_model.pkl`
- Can be integrated into a `Streamlit` app
- Hosted easily using **Streamlit Cloud** for public access

This project is now suitable for:
- Internship submission ‚úÖ
- Portfolio/demo website ‚úÖ
- Real-time prediction apps ‚úÖ
"""

